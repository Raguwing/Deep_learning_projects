{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7df3786",
   "metadata": {},
   "source": [
    "# Face Recognition Using CNN Architecture in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de41a194",
   "metadata": {},
   "source": [
    "> Convolutional Neural Networks(CNN) has changed the way we used to learn images. CNN mimics the way humans see images, by focussing on one portion of the image at a time and scanning the whole image, this is called covolution operation.\n",
    "\n",
    "> CNN boils down every image as a vector of numbers, which can be learned by the fully connected Dense layers of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2074aa65",
   "metadata": {},
   "source": [
    "**In this perticular case study I will be performing how to implement a face recognition model using CNN. You can use this template to create an image classification model on any group of images by putting them in a folder and creating a class.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97ba69",
   "metadata": {},
   "source": [
    "## About the Dataset (Images)\n",
    "\n",
    "> The data contains cropped face images of 16 people divided into Training and testing. We will train the CNN model using the images in the Training folder and then test the model by using the unseen images from the testing folder, to check if the model is able to recognise the face number i.e label given to each face during training of the unseen images or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ce5915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\RAGUWING\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries \n",
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c614ab",
   "metadata": {},
   "source": [
    "# Image Agumentation with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127fef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 16 classes.\n",
      "Found 244 images belonging to 16 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'face1': 0,\n",
       " 'face10': 1,\n",
       " 'face11': 2,\n",
       " 'face12': 3,\n",
       " 'face13': 4,\n",
       " 'face14': 5,\n",
       " 'face15': 6,\n",
       " 'face16': 7,\n",
       " 'face2': 8,\n",
       " 'face3': 9,\n",
       " 'face4': 10,\n",
       " 'face5': 11,\n",
       " 'face6': 12,\n",
       " 'face7': 13,\n",
       " 'face8': 14,\n",
       " 'face9': 15}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = r\"C:\\Users\\RAGUWING\\Desktop\\Resume\\Additional Documents\\Face Images\\Final Training Images\"\n",
    "\n",
    "## Image Preprocessing using keras\n",
    "\n",
    "\n",
    "# As we know deep-learning is hungry for data, the data we have is only limited. \n",
    "# so lets perform **Image Agumentation** to create different versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images.\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_gen = ImageDataGenerator(\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "# No transformations are made on the test data\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "# Generating training data\n",
    "training_data = train_gen.flow_from_directory(\n",
    "    train_images, \n",
    "    target_size = (100,100),\n",
    "    batch_size = 30,\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "# generating test data\n",
    "testing_data = test_gen.flow_from_directory(\n",
    "    train_images, \n",
    "    target_size = (100,100),\n",
    "    batch_size = 30,\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "# Printing class labels for each face\n",
    "testing_data.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe2d7e",
   "metadata": {},
   "source": [
    "**If you observe, the above dictionary is having keys as face_names and values as numbers. We need to swap them because the classifier model will return the answer as the numeric mapping and we need to get the face_name out of it.**\n",
    "\n",
    "> Also, since this is a multi-class classification problem, we are counting the number of unique faces, as that will be used as the number of output neurons in the output layer of fully connected ANN classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fe4e3",
   "metadata": {},
   "source": [
    "## Mapping of class_labels with numeric value for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bde407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data have numeric tag for each face\n",
    "Train_class = training_data.class_indices\n",
    "\n",
    "# lets store them in a dictionary with swap for future reference\n",
    "Result_class = {}\n",
    "for value_tag, face_tag in zip(Train_class.values(),Train_class.keys()):\n",
    "    Result_class[value_tag] = face_tag\n",
    "\n",
    "    \n",
    "# use pickle to save the mapping's\n",
    "import pickle\n",
    "with open(r'C:\\Users\\RAGUWING\\Desktop\\Resume\\Additional Documents\\Face Images\\ResultMap.pkl','wb') as Final_mapping:\n",
    "    pickle.dump(Result_class,Final_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1ce27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its numeric value {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Mapping of Face and its numeric value\",Result_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc02d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Number of output neurons:  16\n"
     ]
    }
   ],
   "source": [
    "Output_Neurons=len(Result_class)\n",
    "print('\\n The Number of output neurons: ', Output_Neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135e5f5",
   "metadata": {},
   "source": [
    "# Building the CNN Architecture, Model Compilation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f06e9",
   "metadata": {},
   "source": [
    "**In the below code snippet, I have created a CNN model with**\n",
    "\n",
    "3. hidden layers of convolution\n",
    "3. hidden layers of max pooling\n",
    "1. layer of flattening\n",
    "2. Hidden ANN layer\n",
    "1. output layer with 16-neurons (one for each face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151a98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8a8a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\RAGUWING\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\RAGUWING\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\RAGUWING\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAGUWING\\AppData\\Local\\Temp\\ipykernel_11848\\45880348.py:51: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  Model.fit_generator(training_data,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\RAGUWING\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\RAGUWING\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "9/9 [==============================] - 3s 198ms/step - loss: 114.9584 - Accuracy: 0.0615 - val_loss: 12.9830 - val_Accuracy: 0.1598\n",
      "Epoch 2/30\n",
      "9/9 [==============================] - 2s 177ms/step - loss: 5.1518 - Accuracy: 0.2500 - val_loss: 2.1260 - val_Accuracy: 0.3607\n",
      "Epoch 3/30\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 2.0824 - Accuracy: 0.4262 - val_loss: 1.6130 - val_Accuracy: 0.5164\n",
      "Epoch 4/30\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 1.5256 - Accuracy: 0.5410 - val_loss: 0.8518 - val_Accuracy: 0.7582\n",
      "Epoch 5/30\n",
      "9/9 [==============================] - 2s 176ms/step - loss: 0.7006 - Accuracy: 0.8033 - val_loss: 0.2181 - val_Accuracy: 0.9344\n",
      "Epoch 6/30\n",
      "9/9 [==============================] - 2s 173ms/step - loss: 0.3223 - Accuracy: 0.9057 - val_loss: 0.0873 - val_Accuracy: 0.9590\n",
      "Epoch 7/30\n",
      "9/9 [==============================] - 2s 172ms/step - loss: 0.2421 - Accuracy: 0.9180 - val_loss: 0.0437 - val_Accuracy: 0.9959\n",
      "Epoch 8/30\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 0.1192 - Accuracy: 0.9590 - val_loss: 0.0117 - val_Accuracy: 0.9959\n",
      "Epoch 9/30\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 0.1119 - Accuracy: 0.9631 - val_loss: 0.0087 - val_Accuracy: 0.9959\n",
      "Epoch 10/30\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 0.0609 - Accuracy: 0.9836 - val_loss: 0.0211 - val_Accuracy: 0.9959\n",
      "Epoch 11/30\n",
      "9/9 [==============================] - 2s 182ms/step - loss: 0.0667 - Accuracy: 0.9754 - val_loss: 0.0091 - val_Accuracy: 0.9959\n",
      "Epoch 12/30\n",
      "9/9 [==============================] - 2s 172ms/step - loss: 0.0629 - Accuracy: 0.9918 - val_loss: 0.0044 - val_Accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "9/9 [==============================] - 2s 176ms/step - loss: 0.0440 - Accuracy: 0.9918 - val_loss: 0.0040 - val_Accuracy: 0.9959\n",
      "Epoch 14/30\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 0.0775 - Accuracy: 0.9754 - val_loss: 0.0190 - val_Accuracy: 0.9959\n",
      "Epoch 15/30\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 0.0957 - Accuracy: 0.9754 - val_loss: 0.0124 - val_Accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "9/9 [==============================] - 2s 176ms/step - loss: 0.0513 - Accuracy: 0.9836 - val_loss: 0.0024 - val_Accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 0.0121 - Accuracy: 0.9959 - val_loss: 0.0053 - val_Accuracy: 1.0000\n",
      "Epoch 17: early stopping\n",
      "Total Training Time taken:  0 Minutes\n"
     ]
    }
   ],
   "source": [
    "'''Initializing the Convolutional Neural Network'''\n",
    "Model = Sequential()\n",
    "\n",
    "\n",
    "''' STEP--1 Convolution\n",
    "# Adding the first layer of CNN\n",
    "# we are using the format (100,100,3) because we are using TensorFlow backend\n",
    "# It means 3 matrix of size (100x100) pixels representing Red, Green and Blue components of pixels\n",
    "'''\n",
    "\n",
    "Model.add(Conv2D(16, kernel_size=(5,5), strides=(1,1), input_shape = (100,100,3),activation='relu'))\n",
    "\n",
    "'''# Maxplooing layer'''\n",
    "\n",
    "Model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''Adding additional layers  of convolution and \n",
    "        maxpooling for better model accuracy and performance'''\n",
    "\n",
    "Model.add(Conv2D(32,kernel_size=(3,3),strides=(1,1),activation='relu'))\n",
    "Model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "'''# Add a flatten layer to convert the vector to one dimensional'''\n",
    "Model.add(Flatten())\n",
    "\n",
    "'''Add dense layers and Initialize weights using \n",
    "                kernal initializer for better learing of image features and classification'''\n",
    "\n",
    "Model.add(Dense(64,activation='relu'))\n",
    "Model.add(Dense(Output_Neurons,activation='softmax'))\n",
    "\n",
    "'''Perform Model Compilation'''\n",
    "\n",
    "Model.compile(loss='categorical_crossentropy',optimizer = 'adam',metrics = ['Accuracy'])\n",
    "\n",
    "\n",
    "'''# Using Early stopping to reduce the training time'''\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "call = EarlyStopping(\n",
    "                    min_delta=0.005,\n",
    "                    patience=5,\n",
    "                     verbose=1\n",
    "                    )\n",
    "\n",
    "import time\n",
    "# Measuring the time taken by the model to train\n",
    "StartTime=time.time()\n",
    "\n",
    "'''# Model Training'''\n",
    "Model.fit_generator(training_data,\n",
    "          epochs = 30,\n",
    "          validation_data=testing_data,\n",
    "                   callbacks=call)\n",
    "\n",
    "Endtime = time.time()\n",
    "print('Total Training Time taken: ',round((Endtime-StartTime)/60),'Minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db111fa",
   "metadata": {},
   "source": [
    "## Using the Classifier to make predictions on unseen test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6412c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce515a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Prediction is:  face4\n"
     ]
    }
   ],
   "source": [
    "'''########### Making single predictions ###########'''\n",
    "\n",
    "ImagePath=r\"C:\\Users\\RAGUWING\\Desktop\\Resume\\Additional Documents\\Face Images\\Final Training Images\\face4\\image_0054_Face_1.jpg\"\n",
    "test_image=image.load_img(ImagePath,target_size=(100, 100))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=Model.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',Result_class[np.argmax(result)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9bd6d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\RAGUWING\\\\Desktop\\\\Resume\\\\Additional Documents\\\\Face Images\\\\Final Testing Images\\\\face1\\\\1face1.jpg', 'C:\\\\Users\\\\RAGUWING\\\\Desktop\\\\Resume\\\\Additional Documents\\\\Face Images\\\\Final Testing Images\\\\face1\\\\2face1.jpg', 'C:\\\\Users\\\\RAGUWING\\\\Desktop\\\\Resume\\\\Additional Documents\\\\Face Images\\\\Final Testing Images\\\\face1\\\\3face1.jpg', 'C:\\\\Users\\\\RAGUWING\\\\Desktop\\\\Resume\\\\Additional Documents\\\\Face Images\\\\Final Testing Images\\\\face1\\\\4face1.jpg', 'C:\\\\Users\\\\RAGUWING\\\\Desktop\\\\Resume\\\\Additional Documents\\\\Face Images\\\\Final Testing Images\\\\face10\\\\1face10.jpg']\n",
      "**************************************************\n",
      "Prediction:  face1\n",
      "Prediction:  face1\n",
      "Prediction:  face13\n",
      "Prediction:  face13\n",
      "Prediction:  face10\n",
      "Prediction:  face10\n",
      "Prediction:  face10\n",
      "Prediction:  face10\n",
      "Prediction:  face11\n",
      "Prediction:  face11\n",
      "Prediction:  face11\n",
      "Prediction:  face11\n",
      "Prediction:  face12\n",
      "Prediction:  face12\n",
      "Prediction:  face12\n",
      "Prediction:  face12\n",
      "Prediction:  face13\n",
      "Prediction:  face13\n",
      "Prediction:  face13\n",
      "Prediction:  face13\n",
      "Prediction:  face14\n",
      "Prediction:  face14\n",
      "Prediction:  face14\n",
      "Prediction:  face14\n",
      "Prediction:  face15\n",
      "Prediction:  face15\n",
      "Prediction:  face15\n",
      "Prediction:  face15\n",
      "Prediction:  face16\n",
      "Prediction:  face16\n",
      "Prediction:  face16\n",
      "Prediction:  face16\n",
      "Prediction:  face2\n",
      "Prediction:  face2\n",
      "Prediction:  face12\n",
      "Prediction:  face2\n",
      "Prediction:  face3\n",
      "Prediction:  face3\n",
      "Prediction:  face3\n",
      "Prediction:  face3\n",
      "Prediction:  face4\n",
      "Prediction:  face4\n",
      "Prediction:  face4\n",
      "Prediction:  face4\n",
      "Prediction:  face5\n",
      "Prediction:  face5\n",
      "Prediction:  face5\n",
      "Prediction:  face5\n",
      "Prediction:  face6\n",
      "Prediction:  face6\n",
      "Prediction:  face12\n",
      "Prediction:  face6\n",
      "Prediction:  face7\n",
      "Prediction:  face7\n",
      "Prediction:  face7\n",
      "Prediction:  face7\n",
      "Prediction:  face8\n",
      "Prediction:  face8\n",
      "Prediction:  face8\n",
      "Prediction:  face8\n",
      "Prediction:  face9\n",
      "Prediction:  face9\n",
      "Prediction:  face9\n",
      "Prediction:  face9\n"
     ]
    }
   ],
   "source": [
    "'''############ Making multiple predictions ###########'''\n",
    "\n",
    "## Loading all the image paths from final testing folder for prediction\n",
    "main_ = r\"C:\\Users\\RAGUWING\\Desktop\\Resume\\Additional Documents\\Face Images\\Final Testing Images\"\n",
    "img_paths = glob.glob(os.path.join(main_,'**','*.jpg'))\n",
    "\n",
    "print(img_paths[0:5]) # every image will be a PIL object\n",
    "print('*'*50)\n",
    "\n",
    "for path in img_paths:\n",
    "    test_image = image.load_img(path,target_size=(100,100))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image,axis =0)\n",
    "    result = Model.predict(test_image,verbose=0)\n",
    "    print('Prediction: ',Result_class[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76020e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e115eb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881db68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4f2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bde496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38438d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7a19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e441cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6312f887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11adaf95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236881b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cb56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e8062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e615c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cd3256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fead7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bfebbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc0901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f6465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ccbc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02ecfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08f67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4210ea41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fafbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa119aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e086824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9986e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d436277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a1ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee375bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeade37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ce898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44804251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7120a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2aa925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71088800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad561665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672afaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579335df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc8386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75467106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7623d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af8fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196869d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2add3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1932cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
